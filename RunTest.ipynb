{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6827,"status":"ok","timestamp":1653576070710,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"},"user_tz":240},"id":"7D-ClLnDbFpR","outputId":"d5c265ea-148a-4c55-b6a4-ad6bd2c09285"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"drive/My Drive/Task1_2inputs\")\n","!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n","import monai\n","from monai.data import  DataLoader, ImageDataset\n","from monai.transforms import (\n","    Resize, NormalizeIntensity, Activations, Compose, EnsureType, CenterSpatialCrop,\n",")\n","\n","from monai.data import decollate_batch\n","from monai.networks.nets import DenseNet121\n","from skimage.morphology import disk, binary_dilation, binary_erosion, remove_small_objects\n","import pandas as pd\n","import numpy as np\n","import nibabel as nib\n","import torch\n","import scipy.ndimage as nd\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import nibabel.processing\n","from monai.utils import set_determinism\n","\n","set_determinism(seed=123)\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["    data_dir = \"data_testing\"\n","    raw_data = pd.read_csv(os.path.join(data_dir, \"Label.csv\"))\n","    for _, c_row in raw_data.iterrows():\n","        patient = str(c_row['filename'].astype(int))\n","        pathCT1 = os.path.join(data_dir, \"baseline\", patient + \".nii.gz\")\n","        pathCT1seg = os.path.join(data_dir, \"mask\", patient + \"-label.nii.gz\")\n","        # load mask and resample\n","        print(\"Processing : \" + pathCT1)\n","        nii = nib.load(pathCT1seg)\n","        print(\"Original\")\n","        print(nii.get_fdata().shape)\n","        matrix = (nii.header[\"pixdim\"])[1:4]\n","        print(matrix)\n","        voxel_size = [1, 1, 1]\n","        fullImage1seg = np.round(nii.get_fdata()).astype(int)\n","        fullImage1seg[fullImage1seg != 1] = 0\n","\n","        affine = np.eye(4)\n","        affine[:3, :3] = np.diag((nii.header[\"pixdim\"])[1:4])\n","        pathtmp = os.path.join(data_dir, \"tmp_brain_seg.nii.gz\")\n","        empty_header = nib.Nifti1Header()\n","        clipped_img = nib.Nifti1Image(fullImage1seg, affine, empty_header)\n","        nib.save(clipped_img, pathtmp)\n","        nii = nib.load(pathtmp)\n","        nii = nibabel.processing.resample_to_output(nii, voxel_size)\n","        fullImage1seg = np.round(nii.get_fdata()).astype(int)\n","        fullImage1seg[fullImage1seg > 0] = 1\n","\n","        # load brain window and resample\n","        window_center, window_width = 40, 80\n","        nii = nib.load(pathCT1)\n","        fullImage1 = nii.get_fdata()\n","        fullImage1[fullImage1 < 0] = 0\n","        fullImage1[fullImage1 > 200] = 0\n","        pathtmp = os.path.join(data_dir, \"tmp_brain_seg.nii.gz\")\n","        empty_header = nib.Nifti1Header()\n","        clipped_img = nib.Nifti1Image(fullImage1, affine, empty_header)\n","        nib.save(clipped_img, pathtmp)\n","        nii = nib.load(pathtmp)\n","        nii = nibabel.processing.resample_to_output(nii, voxel_size)\n","        fullImage1 = nii.get_fdata()\n","        img_min = window_center - window_width // 2\n","        img_max = window_center + window_width // 2\n","        fullImage1[fullImage1 < img_min] = img_min\n","        fullImage1[fullImage1 > img_max] = img_max\n","        fullImage1 = (fullImage1 - fullImage1.min()) / np.ptp(fullImage1)\n","\n","        print(\"Resample (1,1,1)\")\n","        print(fullImage1.shape)\n","        print(fullImage1seg.shape)\n","        # find the first slice which contains hematoma\n","        for slice1 in range(fullImage1seg.shape[2]):\n","            if fullImage1seg[:, :, slice1].sum() > 0:\n","                break\n","        # find the last slice which contains hematoma\n","        for slice2 in range(fullImage1seg.shape[2] - 1, 0, -1):\n","            if fullImage1seg[:, :, slice2].sum() > 0:\n","                break\n","        slice1 = slice1 - 16\n","        if slice1 < 0:\n","            slice1 = 0\n","        slice2 = slice1 + 64\n","        if slice2 > fullImage1seg.shape[2]:\n","            slice2 = fullImage1seg.shape[2]\n","        img1 = np.zeros([fullImage1.shape[0], fullImage1.shape[1], 64])\n","        img1seg = np.zeros([fullImage1seg.shape[0], fullImage1seg.shape[1], 64])\n","        img1[:, :, 0:slice2 - slice1] = fullImage1[:, :, slice1:slice2]\n","        img1seg[:, :, 0:slice2 - slice1] = fullImage1seg[:, :, slice1:slice2]\n","\n","        img1 = 1.0 * (img1 - img1.min()) / np.ptp(img1)\n","        img1[img1 < 0.0001] = 0\n","        img_bw = img1.copy()\n","        img_bw[img_bw > 0] = 1\n","        for slice in range(0, img_bw.shape[2]):\n","            if img_bw[:, :, slice].sum() > 0:\n","                img_bw[:, :, slice] = binary_erosion(img_bw[:, :, slice].astype(np.uint8),\n","                                                     disk(4, dtype=bool))\n","                img_bw[:, :, slice] = remove_small_objects(img_bw[:, :, slice].astype(bool), 500)\n","                img_bw[:, :, slice] = binary_dilation(img_bw[:, :, slice].astype(np.uint8),\n","                                                      disk(4, dtype=bool))\n","                img_bw[:, :, slice] = nd.binary_fill_holes(img_bw[:, :, slice].astype(np.uint8))\n","\n","        img1[img_bw == 0] = 0\n","        img1 = 1.0 * (img1 - img1.min()) / np.ptp(img1)\n","\n","        image = np.zeros([2, img1.shape[0], img1.shape[1], img1.shape[2]])\n","        image[0, :, :, :] = img1\n","        image[1, :, :, :] = img1seg\n","\n","        resizeImage = Resize([192, 192, 64])\n","        cropImage = CenterSpatialCrop(roi_size=(128, 128, 64))\n","        nomalizeImage = NormalizeIntensity()\n","        image = resizeImage(image)\n","        image = nomalizeImage(image)\n","\n","        volume = np.array(image, dtype=np.float32)\n","        # Convert the numpy array into nifti file\n","        volume = nib.Nifti1Image(volume, np.eye(4))\n","        nib.save(volume, os.path.join(data_dir, \"preprocessing\", patient + \"_brain_seg.nii.gz\"))"],"metadata":{"id":"a7h-dJWUS6Ah","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653576501582,"user_tz":240,"elapsed":53311,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}},"outputId":"7523d886-bc1b-429e-a43c-eabb96af936a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing : data_testing/baseline/10028.nii.gz\n","Original\n","(512, 512, 32)\n","[0.46875 0.46875 5.     ]\n","Resample (1,1,1)\n","(241, 241, 156)\n","(241, 241, 156)\n","Processing : data_testing/baseline/10042.nii.gz\n","Original\n","(512, 512, 28)\n","[0.488281  0.488281  4.9998326]\n","Resample (1,1,1)\n","(251, 251, 136)\n","(251, 251, 136)\n","Processing : data_testing/baseline/10051.nii.gz\n","Original\n","(512, 527, 22)\n","[0.468     0.468     5.9917765]\n","Resample (1,1,1)\n","(241, 248, 127)\n","(241, 248, 127)\n","Processing : data_testing/baseline/10052.nii.gz\n","Original\n","(512, 512, 56)\n","[0.48828125 0.48828125 3.        ]\n","Resample (1,1,1)\n","(251, 251, 166)\n","(251, 251, 166)\n","Processing : data_testing/baseline/10056.nii.gz\n","Original\n","(512, 512, 36)\n","[0.488281 0.488281 5.      ]\n","Resample (1,1,1)\n","(251, 251, 176)\n","(251, 251, 176)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVJ9_n8OZSlq","outputId":"fa55a769-5ff7-42bf-e133-a3481359648d","executionInfo":{"status":"ok","timestamp":1653576601187,"user_tz":240,"elapsed":17352,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["AUC total= \n","0.16666666666666666\n"]}],"source":["    root_dir = \"data_testing\"\n","    data_dir = os.path.join(root_dir, \"preprocessing\")\n","    if 1:\n","        raw_data = pd.read_csv(os.path.join(root_dir, \"Label.csv\"))\n","        pathFilenames = []\n","        labels = []\n","        names = []\n","        for _, c_row in raw_data.iterrows():\n","            pathFilenames.append(os.path.join(data_dir, str(c_row['filename']) + \"_brain_seg.nii.gz\"))\n","            labels.append(c_row['label'])\n","            names.append(str(c_row['filename']))\n","        labels = np.asarray(labels).astype(int)\n","        pathFilenames = np.asarray(pathFilenames)\n","        names = np.asarray(names)\n","        if 1:\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            model = monai.networks.nets.DenseNet121(\n","                spatial_dims=3, in_channels=2, out_channels=1, dropout_prob=0.1).to(device)\n","            model.cuda()\n","            model.load_state_dict(\n","                torch.load(\"1_best_metric_model_classification3d_1.pth\"))\n","            model.eval()\n","            xval = pathFilenames\n","            yval = labels\n","            val_ds = ImageDataset(\n","                image_files=xval, labels=yval)\n","            val_loader = DataLoader(val_ds, batch_size=1, shuffle=False,\n","                                    num_workers=1, pin_memory=torch.cuda.is_available())\n","            y_pred_trans = Compose([EnsureType(), Activations(sigmoid=True)])\n","            y_trans = Compose([EnsureType()])\n","            with torch.no_grad():\n","                y_pred = torch.tensor([], dtype=torch.float32, device=device)\n","                y = torch.tensor([], dtype=torch.float32, device=device)\n","                for val_data in val_loader:\n","                    inputs, val_labels = val_data[0].to(\n","                        device), val_data[1].to(device)\n","                    val_outputs = model(inputs)\n","                    y_pred1 = val_outputs.flatten()\n","                    y1 = val_labels\n","                    y_pred = torch.cat([y_pred, y_pred1], dim=0)\n","                    y = torch.cat([y, y1], dim=0)\n","                y_onehot = [y_trans(i) for i in decollate_batch(y)]\n","                y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n","                auc_metric1 = monai.metrics.ROCAUCMetric()\n","                auc_metric1(y_pred_act, y_onehot)\n","                print(\"AUC total= \")\n","                print(auc_metric1.aggregate())\n","                auc_metric1.reset()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"RunTest.ipynb","provenance":[{"file_id":"1WSmJAGEuG1UKaw2gijvNeKBDFAIgsCxI","timestamp":1648200209441}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}