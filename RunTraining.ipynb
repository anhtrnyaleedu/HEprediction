{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8255,"status":"ok","timestamp":1654599012663,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"},"user_tz":240},"id":"7D-ClLnDbFpR","outputId":"c48d9afe-1ed0-40c2-cd63-75ab0020999c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"drive/My Drive/Task1_2inputs\")\n","!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n","import monai\n","from monai.data import  DataLoader, ImageDataset\n","from monai.transforms import (\n","    Resize, NormalizeIntensity, Activations, Compose, EnsureType, CenterSpatialCrop,\n",")\n","\n","from monai.networks.nets import DenseNet121\n","from skimage.morphology import disk, binary_dilation, binary_erosion, remove_small_objects\n","import pandas as pd\n","import numpy as np\n","import nibabel as nib\n","import torch\n","import scipy.ndimage as nd\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import nibabel.processing\n","from monai.utils import set_determinism\n","from monai.data import decollate_batch\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from monai.transforms import (\n","    Compose, Activations,\n","    RandGaussianSmooth,\n","    EnsureType, RandAffine, RandRotate, Resize,  RandZoom, NormalizeIntensity,\n","    CenterSpatialCrop, LoadImage,\n",")\n","from os.path import exists\n","\n","from sklearn.model_selection import StratifiedKFold, KFold\n","import os\n","set_determinism(seed=123)\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["def generate(filenames_brains_segs, labels, output_brains_segs_path, names):\n","    number_runs = 8\n","    for i in range(1, number_runs):\n","        for j in range(0, len(names)):\n","            filepath = filenames_brains_segs[j]\n","            label = labels[j]\n","            if labels[j] == 0:\n","                if i < 5:\n","                    continue\n","            if exists(os.path.join(output_brains_segs_path + str(label),\n","                                  str(names[j]) + \"_\" + str(i) + \"_brain_seg.nii.gz\")) == True:\n","                continue\n","            loader = LoadImage(dtype=np.float32)\n","            image, metadata = loader(filepath)\n","            zoomImage = RandZoom(prob=1, min_zoom=0.7, max_zoom=1.3)\n","            rotateImage = RandRotate(prob=1, range_x=(0.4, 0.4))\n","            gaussImage = RandGaussianSmooth(prob=1, sigma_x=(0.25, 1.5))\n","            affineImage = RandAffine(prob=1, shear_range=(0.2, 0.2), mode=\"bilinear\", padding_mode=\"zeros\")\n","            resizeImage = Resize([192, 192, 64])\n","            cropImage = CenterSpatialCrop(roi_size=(128, 128, 64))\n","            nomalizeImage = NormalizeIntensity()\n","            if i != 8:\n","                image = zoomImage(image)\n","                image = rotateImage(image)\n","                image = gaussImage(image)\n","                image = affineImage(image)\n","            image = resizeImage(image)\n","            image = cropImage(image)\n","            image = nomalizeImage(image)\n","\n","            volume = np.array(image, dtype=np.float32)\n","            # Convert the numpy array into nifti file\n","            volume = nib.Nifti1Image(volume, np.eye(4))\n","            nib.save(volume,\n","                     os.path.join(output_brains_segs_path + str(label),\n","                                  str(names[j]) + \"_\" + str(i) + \"_brain_seg.nii.gz\"))\n","        print(f'step {i} done')\n","\n","def generateVal(filenames_brains_segs, labels, output_brains_segs_path, names):\n","    for j in range(0, len(names)):\n","        filepath = filenames_brains_segs[j]\n","        label = labels[j]\n","        if exists(os.path.join(output_brains_segs_path, str(names[j]) + \"_brain_seg.nii.gz\")) == True:\n","            continue\n","        loader = LoadImage(dtype=np.float32)\n","        image, metadata = loader(filepath)\n","        resizeImage = Resize([192, 192, 64])\n","        cropImage = CenterSpatialCrop(roi_size=(128, 128, 64))\n","        nomalizeImage = NormalizeIntensity()\n","        image = resizeImage(image)\n","        image = cropImage(image)\n","        image = nomalizeImage(image)\n","\n","        volume = np.array(image, dtype=np.float32)\n","        # Convert the numpy array into nifti file\n","        volume = nib.Nifti1Image(volume, np.eye(4))\n","        nib.save(volume, os.path.join(output_brains_segs_path, str(names[j]) + \"_brain_seg.nii.gz\"))\n","if 1:\n","        root_dir = \"data\"\n","        data_dir = os.path.join(root_dir, \"ATACH-2\")\n","        raw_data = pd.read_csv(os.path.join(data_dir, \"Labels_ATACH2_update.csv\"))\n","        raw_data = raw_data.sort_values(by=[\"filename\"], ascending=False)\n","        filenames_brains_segs = []\n","        labels = []\n","        names = []\n","        output_brains_segs_path = 'data/generated_brains_segs_'\n","        output_brains_segs_path_validation = 'data/validation'\n","        for _, c_row in raw_data.iterrows():\n","            filenames_brains_segs.append(\n","                os.path.join(data_dir, \"ATACH2_original\", str(c_row['filename']) + \"_brain_seg.nii.gz\"))\n","            labels.append(c_row['label'])\n","            names.append(c_row['filename'])\n","        labels = np.asarray(labels).astype(int)\n","        filenames_brains_segs = np.asarray(filenames_brains_segs)\n","        generate(filenames_brains_segs, labels, output_brains_segs_path, names)\n","        generateVal(filenames_brains_segs, labels, output_brains_segs_path_validation, names)\n"],"metadata":{"id":"hru48_yovQdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVJ9_n8OZSlq","outputId":"58b16b18-e094-4103-917f-931fe9ce0edf","executionInfo":{"status":"ok","timestamp":1654599850588,"user_tz":240,"elapsed":836927,"user":{"displayName":"Tuan Tran","userId":"03196305730134920723"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training +++ Valdation on fold 1\n","training :\n","(386,)\n","118\n","testing :\n","(194,)\n","59\n","[804 826]\n","----------\n","epoch 1/2\n","epoch 1 average loss: 0.2184\n","current epoch: 1 current AUC: 0.4541---- BEST AUC: 0.4541 ----current VAL LOSS: 0.2702 Best VAL Loss: 0.2702 at epoch 1\n","----------\n","epoch 2/2\n","epoch 2 average loss: 0.2152\n","current epoch: 2 current AUC: 0.5017---- BEST AUC: 0.5017 ----current VAL LOSS: 0.2650 Best VAL Loss: 0.2650 at epoch 2\n","Training +++ Valdation on fold 2\n","training :\n","(387,)\n","118\n","testing :\n","(193,)\n","59\n","[807 826]\n","----------\n","epoch 1/2\n","epoch 1 average loss: 0.2238\n","current epoch: 1 current AUC: 0.5827---- BEST AUC: 0.5827 ----current VAL LOSS: 0.2678 Best VAL Loss: 0.2678 at epoch 1\n","----------\n","epoch 2/2\n","epoch 2 average loss: 0.2202\n","current epoch: 2 current AUC: 0.5899---- BEST AUC: 0.5899 ----current VAL LOSS: 0.2640 Best VAL Loss: 0.2640 at epoch 2\n","Training +++ Valdation on fold 3\n","training :\n","(387,)\n","118\n","testing :\n","(193,)\n","59\n","[807 826]\n","----------\n","epoch 1/2\n","epoch 1 average loss: 0.2267\n","current epoch: 1 current AUC: 0.4484---- BEST AUC: 0.4484 ----current VAL LOSS: 0.2917 Best VAL Loss: 0.2917 at epoch 1\n","----------\n","epoch 2/2\n","epoch 2 average loss: 0.2241\n","current epoch: 2 current AUC: 0.4640---- BEST AUC: 0.4640 ----current VAL LOSS: 0.2882 Best VAL Loss: 0.2882 at epoch 2\n"]}],"source":["    root_dir = \"data\"\n","    data_dir = os.path.join(root_dir, \"ATACH-2\")\n","    if 1:\n","        raw_data = pd.read_csv(os.path.join(data_dir, \"Labels_ATACH2.csv\"))\n","        filenames = []\n","        labels = []\n","        filenamesHead = []\n","        names = []\n","        for _, c_row in raw_data.iterrows():\n","            filenames.append(os.path.join(data_dir, \"ATACH2_original\", str(c_row['filename']) + \"_brain_seg.nii.gz\"))\n","            labels.append(c_row['label'])\n","            names.append(c_row['filename'])\n","        labels = np.asarray(labels).astype(int)\n","        filenames = np.asarray(filenames)\n","        names = np.asarray(names)\n","        skf = StratifiedKFold(n_splits=3, shuffle=False)\n","        fullAUC = 0\n","        for index, (trainval_indices, test_indices) in enumerate(skf.split(filenames, labels)):\n","            print(\"Training +++ Valdation on fold \" + str(index + 1))\n","            # Generate batches from indices\n","            ytrain, yval = labels[trainval_indices], labels[test_indices]\n","            xtrain, xval = filenames[trainval_indices], filenames[test_indices]\n","            nametrain, nameval = names[trainval_indices], names[test_indices]\n","            index_validation = -1\n","            metric_validation = -1\n","            print(\"training :\")\n","            print(ytrain.shape)\n","            print(np.count_nonzero(ytrain))\n","            print(\"testing :\")\n","            print(yval.shape)\n","            print(np.count_nonzero(yval))\n","            if 1:\n","                copyLabel = ytrain.copy()\n","                xtrain = []\n","                ytrain = []\n","                output_path = 'data/generated_brains_segs_'\n","                tmp = len(copyLabel)\n","                for aug in range(1, 8):\n","                    for j in range(0, tmp):\n","                        if copyLabel[j] == 0:\n","                            if aug < 5:\n","                                continue\n","                        xtrain = np.append(xtrain,\n","                                           os.path.join(output_path + str(copyLabel[j]),\n","                                                        str(nametrain[j]) + \"_\" + str(aug) + \"_brain_seg.nii.gz\"))\n","                        ytrain = np.append(ytrain, copyLabel[j])\n","\n","                copyLabel = yval.copy()\n","                xval = []\n","                yval = []\n","                output_path = 'data/validation'\n","                tmp = len(copyLabel)\n","                for j in range(0, tmp):\n","                    xval = np.append(xval, os.path.join(output_path, str(nameval[j]) + \"_brain_seg.nii.gz\"))\n","                    yval = np.append(yval, copyLabel[j])\n","                class_sample_count = np.array([len(np.where(ytrain == t)[0]) for t in np.unique(ytrain)])\n","                print(class_sample_count)\n","                val_ds = ImageDataset(\n","                    image_files=xval, labels=yval)\n","                val_loader = DataLoader(val_ds, batch_size=10, shuffle=False,\n","                                        num_workers=10, pin_memory=torch.cuda.is_available())\n","                train_ds = ImageDataset(\n","                    image_files=xtrain, labels=ytrain)\n","                train_loader = DataLoader(train_ds, batch_size=10, shuffle=True,\n","                                          num_workers=10, pin_memory=torch.cuda.is_available())\n","                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","                model = monai.networks.nets.DenseNet121(\n","                    spatial_dims=3, in_channels=2, out_channels=1, dropout_prob=0.2).to(device)\n","                Loss = monai.losses.FocalLoss(reduction='mean', gamma=2, to_onehot_y=False)\n","                lr = 0.000001\n","                weight_decay = 1e-8\n","                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","                model.cuda()\n","                val_interval = 1\n","                best_loss_metric = 200\n","                best_auc_metric = 0\n","                best_metric_epoch = -1\n","                epoch_loss_values = []\n","                epoch_loss_values_val = []\n","                auc_metric = monai.metrics.ROCAUCMetric()\n","                auc = []\n","                acc = []\n","                y_pred_trans = Compose([EnsureType(), Activations(sigmoid=True)])\n","                y_trans = Compose([EnsureType()])\n","                best_acc = -1\n","                metric_values = []\n","                writer = SummaryWriter()\n","                max_epochs = 2\n","                for epoch in range(max_epochs):\n","                    print(\"-\" * 10)\n","                    print(f\"epoch {epoch + 1}/{max_epochs}\")\n","                    model.train()\n","                    epoch_loss = 0\n","                    step = 0\n","                    for batch_data in train_loader:\n","                        inputs, labels_tmp = batch_data[0].to(device), batch_data[1].to(device)\n","                        optimizer.zero_grad()\n","                        outputs = model(inputs)\n","                        epoch_len = len(train_ds) // train_loader.batch_size\n","                        loss = Loss(torch.sigmoid(outputs), torch.unsqueeze(labels_tmp, 1).float()).cuda()\n","                        step += 1\n","                        epoch_loss += loss.item()\n","                        loss.backward()\n","                        optimizer.step()\n","                    if step == 0:\n","                        continue\n","                    epoch_loss /= step\n","                    epoch_loss_values.append(epoch_loss)\n","                    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n","                    if (epoch + 1) % val_interval == 0:\n","                        model.eval()\n","                        with torch.no_grad():\n","                            num_correct = 0.0\n","                            metric_count = 0\n","                            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n","                            y = torch.tensor([], dtype=torch.long, device=device)\n","                            epoch_loss_val = 0\n","                            step_val = 0\n","                            for val_data in val_loader:\n","                                inputs, val_labels = val_data[0].to(\n","                                    device), val_data[1].to(device)\n","                                val_outputs = model(inputs)\n","                                y_pred1 = val_outputs.flatten()\n","                                y1 = val_labels\n","                                loss_val = Loss(torch.sigmoid(val_outputs),\n","                                                torch.unsqueeze(val_labels, 1).float()).cuda()\n","                                y_pred = torch.cat([y_pred, y_pred1], dim=0)\n","                                y = torch.cat([y, y1], dim=0)\n","                                step_val = step_val + 1\n","                                epoch_loss_val += loss_val.item()\n","                            y_onehot = [y_trans(i) for i in decollate_batch(y)]\n","                            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n","                            auc_metric(y_pred_act, y_onehot)\n","                            auc_result = auc_metric.aggregate()\n","                            auc_metric.reset()\n","                            auc.append(auc_result)\n","                            epoch_loss_val /= step_val\n","                            epoch_loss_values_val.append(epoch_loss_val)\n","                            del y_pred_act, y_onehot\n","                            if epoch_loss_val < best_loss_metric:\n","                                best_loss_metric = epoch_loss_val\n","                                best_metric_epoch = epoch + 1\n","                                torch.save(model.state_dict(),\n","                                           \"best_metric_model_classification3d_\" + str(index + 1) + \".pth\")\n","                            if auc_result >= best_auc_metric:\n","                                best_auc_metric = auc_result\n","                                torch.save(model.state_dict(),\n","                                           \"1_best_metric_model_classification3d_\" + str(index + 1) + \".pth\")\n","                            print(\n","                                \"current epoch: {} current AUC: {:.4f}---- BEST AUC: {:.4f} ----current VAL LOSS: {:.4f} \"\n","                                \"Best VAL Loss: {:.4f} at epoch {}\".format(\n","                                    epoch + 1, auc_result, best_auc_metric, epoch_loss_val, best_loss_metric,\n","                                    best_metric_epoch,\n","                                )\n","                            )"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"RunTraining.ipynb","provenance":[{"file_id":"1WSmJAGEuG1UKaw2gijvNeKBDFAIgsCxI","timestamp":1648200209441}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}